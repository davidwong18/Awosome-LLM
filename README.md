# Awosome-LLM
## Leaderboards
- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- [LLM Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)
- [LLMPerf Leaderboard](https://github.com/ray-project/llmperf-leaderboard)
- [LLM API Hosts Leaderboard](https://artificialanalysis.ai/leaderboards/hosts)
- [LLM Safety Leaderboard (for compressed models)](https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard)
- [MTEB (Massive Text Embedding Benchmark) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
- [BigBench](https://github.com/google/BIG-bench)

## Distributed Training Tools/Frameworks
- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) Ongoing research training transformer models at scale.  ![GitHub Repo stars](https://img.shields.io/github/stars/NVIDIA/Megatron-LM) ![GitHub last commit](https://img.shields.io/github/last-commit/NVIDIA/Megatron-LM)
- [DeepSpeed](https://github.com/microsoft/DeepSpeed) is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.  ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/DeepSpeed) ![GitHub last commit](https://img.shields.io/github/last-commit/microsoft/DeepSpeed)
- [RedCoast(Redco)](https://arxiv.org/pdf/2310.16355) A Lightweight Tool to Automate Distributed Training and Inference. [Code](https://github.com/tanyuqian/redco)  ![GitHub Repo stars](https://img.shields.io/github/stars/tanyuqian/redco) ![GitHub last commit](https://img.shields.io/github/last-commit/tanyuqian/redco)

## Deploy Tools/Frameworks
- [LocalAI](https://github.com/mudler/LocalAI) ![GitHub Repo stars](https://img.shields.io/github/stars/mudler/LocalAI) ![GitHub last commit](https://img.shields.io/github/last-commit/mudler/LocalAI)
- [Ollama](https://github.com/ollama/ollama) ![GitHub Repo stars](https://img.shields.io/github/stars/ollama/ollama) ![GitHub last commit](https://img.shields.io/github/last-commit/ollama/ollama)
- [vLLM](https://github.com/vllm-project/vllm) ![GitHub Repo stars](https://img.shields.io/github/stars/vllm-project/vllm) ![GitHub last commit](https://img.shields.io/github/last-commit/vllm-project/vllm)
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) ![GitHub Repo stars](https://img.shields.io/github/stars/NVIDIA/TensorRT-LLM) ![GitHub last commit](https://img.shields.io/github/last-commit/NVIDIA/TensorRT-LLM)
- [llama.cpp](https://github.com/ggerganov/llama.cpp) ![GitHub Repo stars](https://img.shields.io/github/stars/ggerganov/llama.cpp) ![GitHub last commit](https://img.shields.io/github/last-commit/ggerganov/llama.cpp)
- [LM Studio](https://github.com/lmstudio-ai) 
- [Outlines](https://github.com/outlines-dev/outlines)
- [gpt4all](https://github.com/nomic-ai/gpt4all)
- [gpt4free](https://github.com/xtekky/gpt4free)
- [privateGPT](https://github.com/imartinez/privateGPT)
- [MLC-LLM(C++)](https://github.com/mlc-ai/mlc-llm) Enable everyone to develop, optimize and deploy AI models natively on everyone's devices. ![GitHub Repo stars](https://img.shields.io/github/stars/mlc-ai/mlc-llm) ![GitHub last commit](https://img.shields.io/github/last-commit/mlc-ai/mlc-llm)
- [llamafile](https://github.com/Mozilla-Ocho/llamafile) Distribute and run LLMs with a single file
- [koboldcpp](https://github.com/LostRuins/koboldcpp)
- [exllamav2(C++)](https://github.com/turboderp/exllamav2) A fast inference library for running LLMs locally on modern consumer-class GPUs. ![GitHub Repo stars](https://img.shields.io/github/stars/turboderp/exllamav2) ![GitHub last commit](https://img.shields.io/github/last-commit/turboderp/exllamav2)
- [xinference](https://github.com/xorbitsai/inference) ![GitHub Repo stars](https://img.shields.io/github/stars/xorbitsai/inference) ![GitHub last commit](https://img.shields.io/github/last-commit/xorbitsai/inference)
- [lmdeploy](https://github.com/InternLM/lmdeploy) is a toolkit for compressing, deploying, and serving LLMs
- [FlexGen(Python)](https://github.com/FMInference/FlexGen) Running large language models on a single GPU for throughput-oriented scenarios
- [OpenLLM](https://github.com/bentoml/OpenLLM) Run any open-source LLMs, such as Llama 2, Mistral, as OpenAI compatible API endpoint in the cloud
- [Text Generation Inference](https://github.com/huggingface/text-generation-inference)
- [CTranslate2(C++)](https://github.com/OpenNMT/CTranslate2) fast inference engine for Transformer models in C++. ![GitHub Repo stars](https://img.shields.io/github/stars/OpenNMT/CTranslate2) ![GitHub last commit](https://img.shields.io/github/last-commit/OpenNMT/CTranslate2)
- [DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) MII makes low-latency and high-throughput inference possible, powered by DeepSpeed
- [AirLLM](https://github.com/lyogavin/Anima/tree/main/air_llm)
- [FlexFlow(C++,Python)](https://github.com/flexflow/FlexFlow) Serve is an open-source compiler and distributed system for low latency, high performance LLM serving. ![GitHub Repo stars](https://img.shields.io/github/stars/flexflow/FlexFlow) ![GitHub last commit](https://img.shields.io/github/last-commit/flexflow/FlexFlow)
- [InferFlow(C++)](https://github.com/inferflow/inferflow) is an efficient and highly configurable inference engine for large language models (LLMs). ![GitHub Repo stars](https://img.shields.io/github/stars/inferflow/inferflow) ![GitHub last commit](https://img.shields.io/github/last-commit/inferflow/inferflow)
- [ExeGPT](https://arxiv.org/pdf/2404.07947) Constraint-Aware Resource Scheduling for LLM Inference.


## RAG Frameworks
- [LangChain](https://github.com/langchain-ai/langchain)
- [LlamaIndex](https://github.com/run-llama/llama_index)

## Reduce Input/Output Tokens
- Chunking of input documents
- Compression of input tokens: [LLMLingua Series](https://llmlingua.com/)
- Summarization of input tokens
- Avoid adding few-shot examples
- Limit the length of the output and its formatting

## Model Routing
- [LLamaIndex Routers and LLMSingleSelector](https://docs.llamaindex.ai/en/latest/module_guides/querying/router/#using-selector-as-a-standalone-module)
- [NVIDIA Nemo guardrails](https://github.com/NVIDIA/NeMo-Guardrails)
- [Dynamically route logic based on input with LangChain](https://python.langchain.com/docs/expression_language/how_to/routing)

## Quantization
- [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](https://arxiv.org/pdf/2405.04532) [Code](https://github.com/mit-han-lab/qserve) ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/qserve) ![GitHub last commit](https://img.shields.io/github/last-commit/mit-han-lab/qserve)

## Caching
- [GPTCache](https://github.com/zilliztech/GPTCache)
- [KV-Runahead](https://arxiv.org/pdf/2405.05329) Scalable Causal LLM Inference by Parallel Key-Value Cache Generation.

## Tensor Parallelism
- 202404 [PrimePar: Efficient Spatial-temporal Tensor Partitioning for Large Transformer Model Training](https://dl.acm.org/doi/pdf/10.1145/3620666.3651357)
- 2024   [DISTPAR:TENSOR PARTITIONING FOR DISTRIBUTED NEURAL NETWORK COMPUTING](https://openreview.net/pdf/id=1GdAJ3GsOw)
- 202211 [EFFICIENTLY SCALING TRANSFORMER INFERENCE](https://arxiv.org/pdf/2211.05102)
  
## Heterogeneous Parallel Inference
- [LLM-PQ](https://github.com/tonyzhao-jt/LLM-PQ) Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization.  ![GitHub Repo stars](https://img.shields.io/github/stars/tonyzhao-jt/LLM-PQ) ![GitHub last commit](https://img.shields.io/github/last-commit/tonyzhao-jt/LLM-PQ)
- [HexGen](https://github.com/zhenyutiancandy/HexGen) Serving LLMs on heterogeneous decentralized clusters.
- [MOIRAI](https://github.com/moirai-placement/moirai) Towards Optimal Placement for Distributed Inference on Heterogeneous Devices.
- [HeteGen](https://arxiv.org/pdf/2403.01164): Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices.

## Other Tools
- [LLM AutoEval](https://github.com/mlabonne/llm-autoeval): Automatically evaluate your LLMs using RunPod
- [LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing) Easily merge models using [MergeKit](https://github.com/arcee-ai/mergekit) in one click
- [AutoQuant](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing)    Quantize LLMs in GGUF, GPTQ, EXL2, AWQ, and HQQ formats in one click
- [Model Family Tree](https://colab.research.google.com/drive/1s2eQlolcI1VGgDhqWIANfkfKvcKrMyNr?usp=sharing)    Visualize the family tree of merged models
- [ZeroSpace](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC)    Automatically create a Gradio chat interface using a free ZeroGPU
- [ExLlamaV2](https://github.com/turboderp/exllamav2) [Colab](https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing) Quantize and run EXL2 models and upload them to the HF Hub
- [LMQL](https://lmql.ai/docs/language/overview.html) is a Python-based programming language for LLM programming with declarative elements.

## Other Papers
- [Sarathi-Serve](https://arxiv.org/pdf/2403.02310) Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve.
